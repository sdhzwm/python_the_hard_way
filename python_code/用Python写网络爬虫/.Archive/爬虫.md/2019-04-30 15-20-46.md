
所有网站自身会带有robots.txt和Sitemap文件可以对爬虫有一定的帮助。

#### 1. robots.txt
大多数网站都会定义robots.txt文件，这样可以让爬虫了解爬取该网站时存在哪些限制,在爬取之前，检查robots.txt文件,可以将爬虫被封禁的可能性降至最低，而且还能发现和网站结构相关的线索。

```py
# section 1
User-agent: BadCrawler
Disallow: /

# section 2
User-agent: *
Crawl-delay: 5
Disallow: /trap

# section 3
Sitemap: http://example.python-scraping.com/sitemap.xml

```

在section1中，我们可以看到该网站禁止：BadCrawler的爬虫。

 在section2中，则说明无论使用哪种代理，在两次请求间要有5秒的榨取延时。

在section3中，定义了个Sitemap.

#### Sitemap

sitemap是网站地图，在爬取之前可根据此来查看爬取的内容，而非全部爬取。
虽然Sitemap文件提供了一种爬取网站的有效方式，但是我们仍需对其谨慎处理，因为该文件可能存在缺失、过期或不完整的问题。

#### 寻找网站的所有者

python的第三方库：`WHOIS`

(WHOIS](htt)
