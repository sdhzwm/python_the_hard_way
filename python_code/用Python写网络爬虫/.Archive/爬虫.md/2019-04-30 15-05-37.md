
所有网站自身会带有robots.txt和Sitemap文件可以对爬虫有一定的帮助。

#### 1. robots.txt
大多数网站都会定义robots.txt文件，这样可以让爬虫了解爬取该网站时存在哪些限制,在爬取之前，检查robots.txt文件,可以将爬虫被封禁的可能性降至最低，而且还能发现和网站结构相关的线索。

```py
# section 1
User-agent: BadCrawler
Disallow: /

# section 2
User-agent: *
Crawl-delay: 5
Disallow: /trap

# section 3
Sitemap: http://example.python-scraping.com/sitemap.xml

```

在section1中，我们可以看到该网站禁止：BadCrawler的爬虫。

 在section2中，则说明无论使用哪种代理，在两次请求间要有5秒的榨取延时。

在section1中，我们可以看到该网站禁止：BadCrawler的爬虫。