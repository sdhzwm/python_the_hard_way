
所有网站自身会带有robots.txt和Sitemap文件可以对爬虫有一定的帮助。

#### 1. robots.txt
大多数网站都会定义robots.txt文件，这样可以让爬虫了解爬取该网站时存在哪些限制,在爬取之前，检查robots.txt文件,可以将爬虫被封禁的可能性降至最低，而且还能发现和网站结构相关的线索。

```py
# section 1
User-agent: BadCrawler
Disallow: /

# section 2
User-agent: *
Crawl-delay: 5
Disallow: /trap

# section 3
Sitemap: http://example.python-scraping.com/sitemap.xml

```

在section1中，我们可以看到该网站禁止：BadCrawler的爬虫。

 在section2中，则说明无论使用哪种代理，在两次请求间要有5秒的榨取延时。

在section3中，定义了个Sitemap.

#### Sitemap

sitemap是网站地图，在爬取之前可根据此来查看爬取的内容，而非全部爬取。

“网站地图提供了所有网页的链接，我们会在后面的小节中使用这些信息，用于创建我们的第一个爬虫。虽然Sitemap文件提供了一种爬取网站的有效方式，但是我们仍需对其谨慎处理，因为该文件可能存在缺失、过期或不完整的问题。”

摘录来自: [德] 凯瑟琳•雅姆尔（Katharine Jarmul） [澳] 理查德•劳森（Richard Lawson）. “用Python写网络爬虫（第2版）。” Apple Books. 