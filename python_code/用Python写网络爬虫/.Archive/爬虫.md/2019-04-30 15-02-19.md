
所有网站自身会带有robots.txt和Sitemap文件可以对爬虫有一定的帮助。

#### 1. robots.txt
大多数网站都会定义robots.txt文件，这样可以让爬虫了解爬取该网站时存在哪些限制,在爬取之前，检查robots.txt文件,可以将爬虫被封禁的可能性降至最低，而且还能发现和网站结构相关的线索。
```
“在爬取之前，检查robots.txt文件这一宝贵资源可以将爬虫被封禁的可能性降至最低，而且还能发现和网站结构相关的线索”

摘录来自: [德] 凯瑟琳•雅姆尔（Katharine Jarmul） [澳] 理查德•劳森（Richard Lawson）. “用Python写网络爬虫（第2版）。” Apple Books. 
```
 