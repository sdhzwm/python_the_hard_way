
### 本章重点：

- 分析网页
- 抓取网页的方法
	- 正则表达式
	- Beautiful soup
	- lxml
- 使用控制台
- xpath选择器
- 抓取结果

### 让爬虫从每个网页中抽取一些数据，然后实现某些事情，这种做法被称为抓取

### 1. 分析网页

想要理解一个网页的结构如何，可以使用查看源代码的方法。在大多数浏览器中，都可以在页面上右键单击选择**显示网页源代码**，获取网页的源代码

![](./_image/2019-05-08-11-20-55.png)

对于浏览器的解析，建议使用Chrome或者Firefox浏览器，然后打开其Developer Tools选项。只需右键单击页面上的某个元素（你在抓取时感兴趣的元素），然后选择Inspect Element。

![](./_image/2019-05-08-11-25-51.png)

### 2. 三种网页抓取方法
#### 2.1 正则表达式
[官方文档地址](https://docs.python.org/zh-cn/3/howto/regex.html) 。具体代码不实践了。

#### 2.2  Beautiful Soup

 Beautiful Soup是一个很流行的Python库，可以解析网页，并提供了定位内容的接口。
安装
```pip
pip install beautifulsoup4
```
有些网页不具备良好的HTML格式，如下面HTML就存在属性两侧引号缺失和标签未闭合问题。
```html
<ul class=country>
	<li>Area
	<li>Population
</ul>
```

这样提取数据往往不能得到预期结果，但可以Beautiful Soup来处理。
```python
>>> from bs4 import BeautifulSoup
>>> brocken_html='<ul class=country><Li>Area<li>Population</ul>'
>>> soup = BeautifulSoup(brocken_html,'html.parser')
>>> fixed_html=soup.prettify()
>>> print(fixed_html)
<ul class="country">
 <li>
  Area
  <li>
   Population
  </li>
 </li>
</ul>
>>> 
>>> ul=soup.find('ul',attrs={'class':'country'})
>>> ul.find('li')
<li>Area<li>Population</li></li>
>>> ul.find_all('li')
[<li>Area<li>Population</li></li>, <li>Population</li>]
>>> 
```

[BeautifulSoup官方文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/ )：https://www.crummy.com/software/BeautifulSoup/bs4/doc/ 下面是用BeautifulSoup提取国家面积数据的例子。

# -*- coding: utf-8 -*-

import urllib2
from bs4 import BeautifulSoup

def scrape(html):
    soup = BeautifulSoup(html) 
    tr = soup.find(attrs={'id':'places_area__row'}) # locate the area row
    # 'class' is a special python attribute so instead 'class_' is used
    td = tr.find(attrs={'class':'w2p_fw'})  # locate the area tag
    area = td.text  # extract the area contents from this tag
    return area

if __name__ == '__main__':
    html = urllib2.urlopen('http://example.webscraping.com/view/United-Kingdom-239').read()
    print scrape(html)
虽然BeautifulSoup正则表达式更加复杂，但容易构造和理解，而且无须担心多余空格和标签属性这样布局上的小变化。