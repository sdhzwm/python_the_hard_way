
### 本章重点：

- 分析网页
- 抓取网页的方法
	- 正则表达式
	- Beautiful soup
	- lxml
- 使用控制台
- xpath选择器
- 抓取结果

### 让爬虫从每个网页中抽取一些数据，然后实现某些事情，这种做法被称为抓取

### 1. 分析网页

想要理解一个网页的结构如何，可以使用查看源代码的方法。在大多数浏览器中，都可以在页面上右键单击选择**显示网页源代码**，获取网页的源代码

![](./_image/2019-05-08-11-20-55.png)

对于浏览器的解析，建议使用Chrome或者Firefox浏览器，然后打开其Developer Tools选项。只需右键单击页面上的某个元素（你在抓取时感兴趣的元素），然后选择Inspect Element。

![](./_image/2019-05-08-11-25-51.png)

### 2. 三种网页抓取方法
#### 2.1 正则表达式
[官方文档地址](https://docs.python.org/zh-cn/3/howto/regex.html) 。具体代码不实践了。

#### 2.2  Beautiful Soup

 Beautiful Soup是一个很流行的Python库，可以解析网页，并提供了定位内容的接口。
安装
```pip
pip install beautifulsoup4
```
有些网页不具备良好的HTML格式，如下面HTML就存在属性两侧引号缺失和标签未闭合问题。
```html
<ul class=country>
	<li>Area
	<li>Population
</ul>
```

这样提取数据往往不能得到预期结果，但可以Beautiful Soup来处理。
```python
>>> from bs4 import BeautifulSoup
>>> brocken_html='<ul class=country><Li>Area<li>Population</ul>'
>>> soup = BeautifulSoup(brocken_html,'html.parser')
>>> fixed_html=soup.prettify()
>>> print(fixed_html)
<ul class="country">
 <li>
  Area
  <li>
   Population
  </li>
 </li>
</ul>
>>> 
>>> ul=soup.find('ul',attrs={'class':'country'})
>>> ul.find('li')
<li>Area<li>Population</li></li>
>>> ul.find_all('li')
[<li>Area<li>Population</li></li>, <li>Population</li>]
>>> 
```

[BeautifulSoup官方文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/ )：https://www.crummy.com/software/BeautifulSoup/bs4/doc/ 下面是用BeautifulSoup提取国家面积数据的例子。

```python
from bs4 import BeautifulSoup
import urllib.request

def scrape(html):
	soup = BeautifulSoup(html) 	
	tr = soup.find(attrs={'id':'header'})
	td = tr.find(attrs={'class':'page-header'})  
	area = td.text 
	return area


req = urllib.request.urlopen('http://example.webscraping.com/view/United-Kingdom-239')
html = req.read()
print(scrape(html))

```
### 2.3 Lxml
- - - - - 

Lxml是基于libxml2这个XML解析库的Python封装。该模块用C语言编写的，解析速度比Beautiful Soup更快，不过安装过程也更为复杂。最新的安装说明可以参考http://Lxml.de/installation.html 。 和Beautiful Soup一样，使用lxml模块的第一步也是将有可能不合法的HTML解析为统一格式。


```python
>>> import lxml.html
>>> broken_html='<ul class=country><li>Area<li>Population</ul>'
>>> tree=lxml.html.fromstring(broken_html) #parse the HTML
>>> fixed_html=lxml.html.tostring(tree,pretty_print=True)
>>> print fixed_html
<ul class="country">
<li>Area</li>
<li>Population</li>
</ul>
```
lxml也可以正确解析属性两侧缺失的引号，并闭合标签。解析完输入内容之后，进入选择元素的步骤，此时lxml有几种不用的方法：

XPath选择器（类似Beautiful Soup的find()方法）
CSS选择器（类似jQuery选择器）
这里选用CSS选择器，它更加简洁，也可以用在解析动态内容。

```python
>>> li=tree.cssselect('ul.country > li')[0]
>>> area=li.text_content()
>>> print area
Area
>>> 
```
```table
说明	| 示例
选择所有标签|*
选择<a>标签	|a
选择所有class="link"的标签	|.link
选择class="link"的<a>标签|	a.link
选择id="home"的<a>标签	|a#home
选择父元素为<a>标签的所有<span>标签|	a > span
选择<a>标签内部的所有<span>标签	|a span
选择title属性为"Home"的所有<a>标签	|a[title=Home]
```
解析完输入内容之后，进入选择元素的步骤，lxml可以使用CSS选择器，需要先安装cssselect库，
```pip
pip install cssselect
```
通过对代码树使用cssselect方法，可以利用CSS语法来选择表格中ID为`places_area__row`的行元素，然后是类为`w2p_fw`的子表格数据标签。由于cssselect返回的是一个列表，需要获取其中的第一个结果，并调用text_content方法，以迭代所有子元素并返回每个元素的相关文本。
代码如下：
```python
import lxml.html
import urllib.request

def scrape(html):
	tree = lxml.html.fromstring(html)
	td = tree.cssselect('tr#places_area__row > td.w2p_fw')[0]
	area = td.text_content() 
	return area

if __name__ == '__main__':
	req = urllib.request.urlopen('http://example.webscraping.com/places/default/view/Aland-Islands-2')
	html = req.read()
	print(scrape(html))
```

### 2.4 XPath 选择器

有时候使用CSS选择器无法正常工作，尤其是在HTML非常不完整或存在格式不当的元素时。尽管像BeautifulSoup和lxml这样的库已经尽了最大努力来纠正解析并清理代码，然而它可能还是无法工作，在这些情况下，XPath**可以帮助你基于页面中的层次结构关系构建非常明确的选择器**。

XPath是一种将XML文档的层次结构描述为关系的方式。因为HTML是由XML元素组成的，因此可以使用XPath从HTML文档中定位和选择元素。

```table
选择器描述 | XPath选择器|CSS选择器
选择所有链接|'//a'|'a'
选择类名为"main"的div元素|'//div[@class="main"]'|'div.main'
选择ID为"list"的ul元素|'//ul[@id="list"]'|'ul#list'
从所有段落中选择文本|'//p/text()'|'p'*”
“选择所有类名中包含'test'的div元素|'//div[contains(@class, 'test')]'|'div [class*="test"]'
选择所有包含链接或列表的div元素|'//div[a|ul] '|'div a, div ul'
选择href属性中包含google.com的链接|'//a[contains(@href, "google.com")]|'a'*”

```

```
```






